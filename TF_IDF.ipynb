{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siriwatsc-debug/FinalProject-ML/blob/main/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhNsYLnaZrTJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report,confusion_matrix\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HashingVectorizer"
      ],
      "metadata": {
        "id": "5yXJBiO8I4XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "file_path = \"https://github.com/siriwatsc-debug/FinalProject-ML/raw/main/train.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "df['Combined'] = df['Subject'].fillna('') + \" \" + df['Body'].fillna('')\n",
        "df['Label_Binary'] = df['Label'].apply(lambda x: 1 if x == \"Phishing\" else 0)"
      ],
      "metadata": {
        "id": "mXSAUyBBZ0Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['Combined'],\n",
        "    df['Label_Binary'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        "    stratify=df['Label_Binary'])"
      ],
      "metadata": {
        "id": "S_sUf8LzZ-Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"hash\", HashingVectorizer(\n",
        "        n_features=2**18,     # large enough for email vocabulary\n",
        "        alternate_sign=False, # prevents negative values\n",
        "        ngram_range=(1, 2),   # unigrams + bigrams (safe)\n",
        "        norm=\"l2\"\n",
        "    )),\n",
        "    (\"svd\", TruncatedSVD(n_components=300, random_state=42)),  # PCA-like step\n",
        "    (\"scale\", StandardScaler()),\n",
        "    (\"svm\", SGDClassifier(\n",
        "        loss=\"hinge\",         # linear SVM\n",
        "        random_state=42\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "6OocY_yBaJZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. GridSearchCV (Leak-Proof Hyperparameter Tuning)\n",
        "param_grid = {\n",
        "    \"svm__alpha\": [1e-5, 1e-4, 1e-3],\n",
        "    \"svm__max_iter\": [2000]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    scoring=\"f1\"\n",
        ")\n",
        "\n",
        "print(\"Training leak-proof SVM model...\")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest parameters:\", grid.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-HQh16maLkn",
        "outputId": "e2277374-986b-47d0-b706-9da4753d7170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training leak-proof SVM model...\n",
            "\n",
            "Best parameters: {'svm__alpha': 0.001, 'svm__max_iter': 2000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Evaluate on *true* unseen test data\n",
        "# -------------------------------------------------\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\n=== True Leak-Proof Evaluation ===\")\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
        "print(\"Precision:\", round(precision_score(y_test, y_pred), 4))\n",
        "print(\"Recall:\", round(recall_score(y_test, y_pred), 4))\n",
        "print(\"F1 Score:\", round(f1_score(y_test, y_pred), 4))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4-6jcAoaO0e",
        "outputId": "6904c50e-ef7c-4f2c-ea8a-cb70eabe2664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== True Leak-Proof Evaluation ===\n",
            "Accuracy: 0.85\n",
            "Precision: 0.7692\n",
            "Recall: 1.0\n",
            "F1 Score: 0.8696\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.70      0.82        10\n",
            "           1       0.77      1.00      0.87        10\n",
            "\n",
            "    accuracy                           0.85        20\n",
            "   macro avg       0.88      0.85      0.85        20\n",
            "weighted avg       0.88      0.85      0.85        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_URL = \"https://github.com/siriwatsc-debug/FinalProject-ML/raw/main/test.csv\"\n",
        "df_test = pd.read_csv(TEST_URL)\n",
        "df_test[\"Combined\"] = df_test[\"Subject\"].fillna(\"\") + \" \" + df_test[\"Body\"].fillna(\"\")\n",
        "df_test[\"Label_Binary\"] = df_test[\"Label\"].apply(lambda x: 1 if x == \"Phishing\" else 0)\n",
        "\n",
        "X_test = df_test[\"Combined\"]\n",
        "y_test = df_test[\"Label_Binary\"]\n",
        "\n",
        "print(f\"Test data shape: {df_test.shape}\")\n",
        "print(\"Class distribution:\\n\", df_test[\"Label_Binary\"].value_counts())\n",
        "\n",
        "# ============================================\n",
        "# 5. Predict & Evaluate\n",
        "# ============================================\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy  = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall    = recall_score(y_test, y_pred)\n",
        "f1        = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\n========== LEAK-PROOF TEST RESULTS ==========\")\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall   : {recall:.4f}\")\n",
        "print(f\"F1-Score : {f1:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Legitimate (0)\", \"Phishing (1)\"]))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naEM5p-7b6A-",
        "outputId": "0b53546b-631b-4700-e3ec-e7909082f1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data shape: (26, 5)\n",
            "Class distribution:\n",
            " Label_Binary\n",
            "0    13\n",
            "1    13\n",
            "Name: count, dtype: int64\n",
            "\n",
            "========== LEAK-PROOF TEST RESULTS ==========\n",
            "Accuracy : 0.8462\n",
            "Precision: 0.8000\n",
            "Recall   : 0.9231\n",
            "F1-Score : 0.8571\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Legitimate (0)       0.91      0.77      0.83        13\n",
            "  Phishing (1)       0.80      0.92      0.86        13\n",
            "\n",
            "      accuracy                           0.85        26\n",
            "     macro avg       0.85      0.85      0.85        26\n",
            "  weighted avg       0.85      0.85      0.85        26\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  3]\n",
            " [ 1 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[\"Predicted_Label\"] = y_pred\n",
        "df_test[\"Predicted_Label_Text\"] = df_test[\"Predicted_Label\"].apply(lambda x: \"Phishing\" if x == 1 else \"Legitimate\")\n",
        "\n",
        "# ============================================\n",
        "# 7. Export Test Dataset with Predictions\n",
        "# ============================================\n",
        "results_csv_path = \"test_with_predictions.csv\"\n",
        "df_test.to_csv(results_csv_path, index=False)\n",
        "print(f\"\\n✓ Test dataset with predictions exported to {results_csv_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdCn6j3mcPTU",
        "outputId": "0fc29acf-f426-40e5-d776-df754ffe26c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Test dataset with predictions exported to test_with_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TF-IDF"
      ],
      "metadata": {
        "id": "huJCK5DYIz7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF -> TruncatedSVD (PCA for sparse) -> SVM\n",
        "# Cleaned, debugged, with GridSearchCV and unseen evaluation\n",
        "\n",
        "import re\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, classification_report, confusion_matrix)\n",
        "\n",
        "# -----------------------\n",
        "# Config / URLs / Seeds\n",
        "# -----------------------\n",
        "TRAIN_URL = \"https://github.com/siriwatsc-debug/FinalProject-ML/raw/main/train.csv\"\n",
        "UNSEEN_URL = \"https://github.com/siriwatsc-debug/FinalProject-ML/raw/main/test.csv\"\n",
        "RANDOM_STATE = 42\n",
        "MODEL_OUT_PATH = \"svm_tfidf_svd_pipeline.joblib\"\n",
        "\n",
        "# -----------------------\n",
        "# Utility: text combine & cleaning\n",
        "# -----------------------\n",
        "def combine_and_clean(subject, body):\n",
        "    # Combine subject and body and do light cleaning to match training assumptions\n",
        "    subj = \"\" if pd.isna(subject) else str(subject)\n",
        "    body = \"\" if pd.isna(body) else str(body)\n",
        "    txt = f\"{subj} {body}\".strip()\n",
        "    # lower, replace urls and emails (keeps content distribution similar)\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(r\"http\\S+|www\\S+\", \" url \", txt)\n",
        "    txt = re.sub(r\"\\S+@\\S+\", \" email \", txt)\n",
        "    # optionally remove non-alphanumeric except spaces (keep this if needed)\n",
        "    txt = re.sub(r\"[^a-z0-9\\s]\", \" \", txt)\n",
        "    # normalize spaces\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "    return txt\n",
        "\n",
        "# -----------------------\n",
        "# 1. Load data\n",
        "# -----------------------\n",
        "print(\"=\"*80)\n",
        "print(\"Loading training data...\")\n",
        "print(\"=\"*80)\n",
        "df = pd.read_csv(TRAIN_URL)\n",
        "\n",
        "# create combined text column\n",
        "# The above ugly one-liner is only to avoid linter issues in some editors. Replace with simple:\n",
        "df[\"Combined_Text\"] = (df[\"Subject\"].fillna(\"\") + \" \" + df[\"Body\"].fillna(\"\")).apply(lambda x: combine_and_clean(x, \"\"))\n",
        "\n",
        "# Label encoding: Legitimate -> 0, else -> 1\n",
        "df[\"Label_Binary\"] = df[\"Label\"].apply(lambda x: 0 if str(x).strip().lower() == \"legitimate\" else 1)\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"Class distribution:\")\n",
        "print(df[\"Label\"].value_counts())\n",
        "\n",
        "# -----------------------\n",
        "# 2. Train/test split\n",
        "# -----------------------\n",
        "X = df[\"Combined_Text\"].values\n",
        "y = df[\"Label_Binary\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "print(\"\\nTrain / Test sizes:\", X_train.shape[0], X_test.shape[0])\n",
        "\n",
        "# -----------------------\n",
        "# 3. Pipeline: TF-IDF -> TruncatedSVD -> StandardScaler -> SVM\n",
        "# -----------------------\n",
        "pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(max_df=0.95, min_df=3, ngram_range=(1,2), stop_words=\"english\")),\n",
        "    (\"svd\", TruncatedSVD(n_components=200, random_state=RANDOM_STATE)),\n",
        "    (\"scaler\", StandardScaler()),   # TruncatedSVD yields dense; scale helps SVM\n",
        "    (\"svm\", SVC(probability=True, random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "# -----------------------\n",
        "# 4. Hyperparameter grid\n",
        "# -----------------------\n",
        "# We include n_components in the GridSearch so SVD dims are tuned too.\n",
        "param_grid = {\n",
        "    \"svd__n_components\": [100, 200, 300],     # adjust based on your dataset size\n",
        "    \"svm__kernel\": [\"linear\", \"rbf\"],\n",
        "    \"svm__C\": [0.1, 1, 10],\n",
        "    \"svm__gamma\": [\"scale\", \"auto\"]           # used when kernel='rbf'\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 5. Train (Grid search)\n",
        "# -----------------------\n",
        "print(\"\\nStarting GridSearchCV training (this may take a while)...\")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest params:\", grid.best_params_)\n",
        "print(\"Best CV score:\", grid.best_score_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# cross-val on full train for sanity\n",
        "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
        "print(\"Cross-val accuracies on training set:\", cv_scores)\n",
        "print(\"Mean CV accuracy:\", np.mean(cv_scores), \"+/-\", np.std(cv_scores))\n",
        "\n",
        "# -----------------------\n",
        "# 6. Evaluate on hold-out test set\n",
        "# -----------------------\n",
        "print(\"\\nEvaluating on hold-out test set...\")\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_prob = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, \"predict_proba\") else None\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, zero_division=0))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred, zero_division=0))\n",
        "print(\"F1:\", f1_score(y_test, y_pred, zero_division=0))\n",
        "if y_prob is not None:\n",
        "    try:\n",
        "        print(\"AUC:\", roc_auc_score(y_test, y_prob))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Legitimate\", \"Phishing\"]))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# -----------------------\n",
        "# 7. Save the final model and vectorizer+svd pipeline\n",
        "# -----------------------\n",
        "joblib.dump(best_model, MODEL_OUT_PATH)\n",
        "print(f\"\\nSaved best pipeline to: {MODEL_OUT_PATH}\")\n",
        "\n",
        "# Save scaler/others separately if needed (not necessary because pipeline contains everything)\n",
        "# joblib.dump(scaler, \"feature_scaler.pkl\")\n",
        "\n",
        "# -----------------------\n",
        "# 8. Evaluate on unseen data (test.csv)\n",
        "# -----------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Loading unseen data and evaluating saved model...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    df_unseen = pd.read_csv(UNSEEN_URL)\n",
        "    df_unseen[\"Combined_Text\"] = (df_unseen[\"Subject\"].fillna(\"\") + \" \" + df_unseen[\"Body\"].fillna(\"\")).apply(lambda x: combine_and_clean(x, \"\"))\n",
        "    df_unseen[\"Label_Binary\"] = df_unseen[\"Label\"].apply(lambda x: 0 if str(x).strip().lower() == \"legitimate\" else 1)\n",
        "\n",
        "    X_unseen = df_unseen[\"Combined_Text\"].values\n",
        "    y_unseen = df_unseen[\"Label_Binary\"].values\n",
        "\n",
        "    # Load model (we already have best_model in memory, but show loading example)\n",
        "    loaded = joblib.load(MODEL_OUT_PATH)\n",
        "\n",
        "    y_pred_unseen = loaded.predict(X_unseen)\n",
        "    y_prob_unseen = loaded.predict_proba(X_unseen)[:, 1] if hasattr(loaded, \"predict_proba\") else None\n",
        "\n",
        "    acc_u = accuracy_score(y_unseen, y_pred_unseen)\n",
        "    prec_u = precision_score(y_unseen, y_pred_unseen, zero_division=0)\n",
        "    rec_u = recall_score(y_unseen, y_pred_unseen, zero_division=0)\n",
        "    f1_u = f1_score(y_unseen, y_pred_unseen, zero_division=0)\n",
        "\n",
        "    print(f\"\\nUnseen set performance - Accuracy: {acc_u:.4f} Precision: {prec_u:.4f} Recall: {rec_u:.4f} F1: {f1_u:.4f}\")\n",
        "    if y_prob_unseen is not None:\n",
        "        try:\n",
        "            auc_u = roc_auc_score(y_unseen, y_prob_unseen)\n",
        "            print(f\"AUC (unseen): {auc_u:.4f}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(\"\\nClassification Report (unseen):\")\n",
        "    print(classification_report(y_unseen, y_pred_unseen, target_names=[\"Legitimate\", \"Phishing\"]))\n",
        "\n",
        "    print(\"Confusion Matrix (unseen):\")\n",
        "    print(confusion_matrix(y_unseen, y_pred_unseen))\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error loading or evaluating unseen data:\", e)\n"
      ],
      "metadata": {
        "id": "pnbnEHxbc9Wj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d542da08-aae2-420f-f9eb-63e7be54c283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Loading training data...\n",
            "================================================================================\n",
            "Dataset shape: (100, 5)\n",
            "Class distribution:\n",
            "Label\n",
            "Legitimate    50\n",
            "Phishing      50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train / Test sizes: 80 20\n",
            "\n",
            "Starting GridSearchCV training (this may take a while)...\n",
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "\n",
            "Best params: {'svd__n_components': 100, 'svm__C': 0.1, 'svm__gamma': 'scale', 'svm__kernel': 'linear'}\n",
            "Best CV score: 0.9875\n",
            "Cross-val accuracies on training set: [1.     1.     1.     0.9375 1.    ]\n",
            "Mean CV accuracy: 0.9875 +/- 0.024999999999999998\n",
            "\n",
            "Evaluating on hold-out test set...\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1: 1.0\n",
            "AUC: 0.0\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Legitimate       1.00      1.00      1.00        10\n",
            "    Phishing       1.00      1.00      1.00        10\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 10]]\n",
            "\n",
            "Saved best pipeline to: svm_tfidf_svd_pipeline.joblib\n",
            "\n",
            "================================================================================\n",
            "Loading unseen data and evaluating saved model...\n",
            "================================================================================\n",
            "\n",
            "Unseen set performance - Accuracy: 1.0000 Precision: 1.0000 Recall: 1.0000 F1: 1.0000\n",
            "AUC (unseen): 0.0000\n",
            "\n",
            "Classification Report (unseen):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Legitimate       1.00      1.00      1.00        13\n",
            "    Phishing       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        26\n",
            "   macro avg       1.00      1.00      1.00        26\n",
            "weighted avg       1.00      1.00      1.00        26\n",
            "\n",
            "Confusion Matrix (unseen):\n",
            "[[13  0]\n",
            " [ 0 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21vFCYW8HRA6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}